{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-18T22:02:20.561413Z",
     "end_time": "2023-04-18T22:02:47.397770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cubix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cubix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def save_tokenized_sentences(tokenized_sentences, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(tokenized_sentences, file)\n",
    "\n",
    "def load_tokenized_sentences(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        tokenized_sentences = pickle.load(file)\n",
    "    return tokenized_sentences\n",
    "\n",
    "def read_tokenized_sentences(folder_path):\n",
    "\n",
    "    data_path = pathlib.Path(folder_path)\n",
    "    paths = list(data_path.iterdir())\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for path in tqdm(paths, desc=folder_path):\n",
    "        for name in os.listdir(path):\n",
    "            if name == \"text_structure.xml\":\n",
    "                file_path = os.path.join(path, name)\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                elements = root.findall(\".//{http://www.tei-c.org/ns/1.0}p\")\n",
    "                for element in elements:\n",
    "                    try:\n",
    "                        text = \"\".join(element.text)\n",
    "                        tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "                        sentences = tokenizer.tokenize(text)\n",
    "                        for sentence in sentences:\n",
    "                            tokenized_sentences.append(sentence.split())\n",
    "                        # for sentence in sent_tokenize(text, language=\"polish\"):\n",
    "                        #     tokenized_sentences.append(word_tokenize(sentence, language=\"polish\"))\n",
    "\n",
    "                    except TypeError:\n",
    "                        pass\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "def preprocess_text(sentences):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation) # removing punctuation - it has to be a conscious decision\n",
    "    tokenized_sentences = [[word.lower().translate(table) for word in sentence] for sentence in sentences]\n",
    "    new_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        new_sentence = []\n",
    "        for x in sentence:\n",
    "            if x != '':\n",
    "                new_sentence.append(x)\n",
    "        new_tokenized_sentences.append(new_sentence)\n",
    "        # new_tokenized_sentences.append(' '.join(new_sentence))\n",
    "\n",
    "    removed_empty_sentences = []\n",
    "    for sentence in new_tokenized_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            removed_empty_sentences.append(sentence)\n",
    "\n",
    "    return removed_empty_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T22:02:47.370768Z",
     "end_time": "2023-04-18T22:02:47.463108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nkjp-wikipedia: 100%|██████████| 634/634 [05:08<00:00,  2.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "6310933"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_sentences = read_tokenized_sentences('nkjp-wikipedia')\n",
    "len(wikipedia_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:12:05.940531Z",
     "end_time": "2023-04-18T21:17:15.978272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5SCAL-free/4/IJP/_internet/senat/xml/k5: 100%|██████████| 29/29 [00:00<00:00, 72.86it/s]\n",
      "5SCAL-free/4/IJP/_internet/senat/xml/k6: 100%|██████████| 39/39 [00:00<00:00, 63.93it/s]\n",
      "5SCAL-free/4/IJP/_internet/senat/xml/k7: 100%|██████████| 54/54 [00:00<00:00, 69.44it/s]\n",
      "5SCAL-free/KONWERTOWANE/IPI/KomisjaSledczaRywin: 100%|██████████| 89/89 [00:01<00:00, 71.08it/s]\n",
      "5SCAL-free/KONWERTOWANE/IPI/KSIAZKI-popr: 100%|██████████| 2/2 [00:00<00:00, 11.98it/s]\n",
      "5SCAL-free/KONWERTOWANE/IPI/law2-agr: 100%|██████████| 50/50 [00:25<00:00,  1.99it/s]\n",
      "5SCAL-free/KONWERTOWANE/IPI/sejm: 100%|██████████| 968/968 [00:28<00:00, 34.25it/s]\n",
      "5SCAL-free/KONWERTOWANE/IPI/senat: 100%|██████████| 553/553 [00:11<00:00, 46.19it/s]\n",
      "5SCAL-free/KONWERTOWANE/Lodz/pelcra/pelcra_xml/misc-published/sejmowe: 100%|██████████| 44/44 [00:00<00:00, 59.60it/s]\n",
      "5SCAL-free/KONWERTOWANE/Lodz/pelcra/pelcra_xml/misc-published/ustawy: 100%|██████████| 11/11 [00:00<00:00, 12.02it/s]\n",
      "5SCAL-free/KONWERTOWANE/Lodz/pelcra/pelcra_xml/transcript: 100%|██████████| 50/50 [00:01<00:00, 43.22it/s]\n"
     ]
    }
   ],
   "source": [
    "scal_paths = [\n",
    "    '5SCAL-free/4/IJP/_internet/senat/xml/k5',\n",
    "    '5SCAL-free/4/IJP/_internet/senat/xml/k6',\n",
    "    '5SCAL-free/4/IJP/_internet/senat/xml/k7',\n",
    "    '5SCAL-free/KONWERTOWANE/IPI/KomisjaSledczaRywin',\n",
    "    '5SCAL-free/KONWERTOWANE/IPI/KSIAZKI-popr',\n",
    "    '5SCAL-free/KONWERTOWANE/IPI/law2-agr',\n",
    "    '5SCAL-free/KONWERTOWANE/IPI/sejm',\n",
    "    '5SCAL-free/KONWERTOWANE/IPI/senat',\n",
    "    '5SCAL-free/KONWERTOWANE/Lodz/pelcra/pelcra_xml/misc-published/sejmowe',\n",
    "    '5SCAL-free/KONWERTOWANE/Lodz/pelcra/pelcra_xml/misc-published/ustawy',\n",
    "    '5SCAL-free/KONWERTOWANE/Lodz/pelcra/pelcra_xml/transcript'\n",
    "]\n",
    "\n",
    "scal_sentences = []\n",
    "\n",
    "for folder in scal_paths:\n",
    "    scal_sentences += read_tokenized_sentences(folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:17:15.983272Z",
     "end_time": "2023-04-18T21:18:27.496368Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "573561"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scal_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:18:27.497369Z",
     "end_time": "2023-04-18T21:18:27.540486Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "wikipedia_sentences = preprocess_text(wikipedia_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:18:27.513488Z",
     "end_time": "2023-04-18T21:23:59.111900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "5767033"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikipedia_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:23:59.070900Z",
     "end_time": "2023-04-18T21:23:59.182897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "scal_sentences = preprocess_text(scal_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:23:59.077900Z",
     "end_time": "2023-04-18T21:24:06.403897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "445267"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scal_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:24:06.393900Z",
     "end_time": "2023-04-18T21:24:06.404898Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "['14',\n 'stycznia',\n '2003',\n 'r',\n 'komisja',\n 'śledcza',\n 'do',\n 'zbadania',\n 'ujawnionych',\n 'w',\n 'mediach',\n 'zarzutów',\n 'dotyczących',\n 'przypadków',\n 'korupcji',\n 'podczas',\n 'prac',\n 'nad',\n 'nowelizacją',\n 'ustawy',\n 'o',\n 'radiofonii',\n 'i',\n 'telewizji',\n 'obradująca',\n 'pod',\n 'przewodnictwem',\n 'marszałka',\n 'sejmu',\n 'marka',\n 'borowskiego',\n 'dokonała']"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scal_sentences[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:24:06.407900Z",
     "end_time": "2023-04-18T21:24:06.424898Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "['awk',\n 'jest',\n 'interpretowanym',\n 'językiem',\n 'programowania',\n 'którego',\n 'główną',\n 'funkcją',\n 'jest',\n 'wyszukiwanie',\n 'i',\n 'przetwarzanie',\n 'wzorców']"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_sentences[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:24:06.422903Z",
     "end_time": "2023-04-18T21:24:06.623920Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "save_tokenized_sentences(scal_sentences, 'pickles/scal_tokenized_sentences.pkl')\n",
    "save_tokenized_sentences(wikipedia_sentences, 'pickles/wikipedia_tokenized_sentences.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:24:06.470172Z",
     "end_time": "2023-04-18T21:24:36.347099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m scal_sentences \u001B[38;5;241m=\u001B[39m load_tokenized_sentences(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickles/scal_tokenized_sentences.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m wikipedia_sentences \u001B[38;5;241m=\u001B[39m \u001B[43mload_tokenized_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpickles/wikipedia_tokenized_sentences.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m tokenized_sentences \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      5\u001B[0m tokenized_sentences\u001B[38;5;241m.\u001B[39mextend(scal_sentences)\n",
      "Cell \u001B[1;32mIn[2], line 7\u001B[0m, in \u001B[0;36mload_tokenized_sentences\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_tokenized_sentences\u001B[39m(file_path):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m----> 7\u001B[0m         tokenized_sentences \u001B[38;5;241m=\u001B[39m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_sentences\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "scal_sentences = load_tokenized_sentences('pickles/scal_tokenized_sentences.pkl')\n",
    "wikipedia_sentences = load_tokenized_sentences('pickles/wikipedia_tokenized_sentences.pkl')\n",
    "\n",
    "tokenized_sentences = []\n",
    "tokenized_sentences.extend(scal_sentences)\n",
    "tokenized_sentences.extend(wikipedia_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:26:44.746234Z",
     "end_time": "2023-04-18T21:27:14.672830Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "6212300"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T21:27:14.676828Z",
     "end_time": "2023-04-18T21:27:14.687827Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Word-level generation and prediction\n",
    "# Toy-problem, not using the previous corpuses\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the Word2Vec model and text corpus data\n",
    "word2vec_model = Word2Vec.load(\"word2vec_large_model.model\")\n",
    "# corpus = ['This is the second sentence to train the model', 'This is the first sentence', 'This is the third sentence just because']\n",
    "corpus = tokenized_sentences\n",
    "embedding_dim = 100\n",
    "\n",
    "\n",
    "def extract_target_words(corpus):\n",
    "    target_words = []\n",
    "    for sentence in corpus:\n",
    "        # words = sentence.split()\n",
    "        target_word = sentence[-1]  # Get the last word in the sentence\n",
    "        target_words.append(target_word)\n",
    "    return target_words\n",
    "\n",
    "target_words = extract_target_words(corpus)\n",
    "\n",
    "# Create target_vectors\n",
    "target_vectors = np.zeros((len(target_words), embedding_dim))\n",
    "\n",
    "for idx, word in enumerate(target_words):\n",
    "    try:\n",
    "        target_vectors[idx] = word2vec_model.wv[word]\n",
    "    except KeyError:\n",
    "        target_vectors[idx] = np.random.rand(embedding_dim)  # initialize randomly\n",
    "\n",
    "# Transform each word in the corpus into a corresponding vector representation\n",
    "corpus_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_vectors = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sentence_vectors.append(word2vec_model.wv[word])\n",
    "        except KeyError:\n",
    "            sentence_vectors.append(np.random.rand(100))  # initialize randomly\n",
    "    corpus_vectors.append(sentence_vectors)\n",
    "\n",
    "# Pad each sentence in the corpus to a fixed length, as required by the LSTM model\n",
    "'''\n",
    "Padding is performed by adding filler values (usually zeros) to the sequences until they reach the desired length.\n",
    "Padding is necessary for LSTMs (Long Short-Term Memory) that process input sequences of a fixed length.\n",
    "\n",
    "\n",
    "There are two common padding strategies:\n",
    "Pre-padding: Filler values are added to the beginning of the sequence.\n",
    "Post-padding: Filler values are added to the end of the sequence.\n",
    "\n",
    "The max_len variable is set to the length of the longest sequence in the corpus. B\n",
    "y padding all sequences to this length, we ensure that the input to the LSTM model is\n",
    "standardized and can be processed correctly.\n",
    "\n",
    "It's important to note that padding can affect model performance.\n",
    "Very short sequences may be dominated by the padding values, which can introduce\n",
    "noise and make it harder for the model to learn meaningful patterns.\n",
    "On the other hand, excessively long padding can lead to increased computational\n",
    "requirements and memory consumption.\n",
    "Choosing an appropriate padding length based on the dataset and problem is crucial for achieving good performance.\n",
    "'''\n",
    "\n",
    "max_len = max(len(sentence) for sentence in corpus_vectors)\n",
    "corpus_vectors_padded = np.zeros((len(corpus_vectors), max_len, embedding_dim), dtype='float32')\n",
    "\n",
    "for i, sentence_vectors in enumerate(corpus_vectors):\n",
    "    for j, vector in enumerate(sentence_vectors):\n",
    "        corpus_vectors_padded[i, j, :] = vector\n",
    "\n",
    "# Build an LSTM model in Keras with appropriate input and output layers\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len, embedding_dim)))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(embedding_dim))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the LSTM model on the prepared data\n",
    "model.fit(corpus_vectors_padded, target_vectors, epochs=1, batch_size=32, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
