{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:31:02.224070Z",
     "end_time": "2023-04-05T18:31:03.663879Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras import layers\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def preprocess_text(sentences):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)  # removing punctuation - it has to be a conscious decision\n",
    "    tokenized_sentences = [[word.lower().translate(table) for word in sentence] for sentence in sentences]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokenized_sentences = [[word for word in sentence if word and word not in stop_words] for sentence in\n",
    "                           tokenized_sentences]\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def save_tokenized_sentences(tokenized_sentences, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(tokenized_sentences, file)\n",
    "\n",
    "\n",
    "def load_tokenized_sentences(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        tokenized_sentences = pickle.load(file)\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def preprocess_text_unpacked(sentences):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)  # removing punctuation - it has to be a conscious decision\n",
    "    tokenized_sentences = [[word.lower().translate(table) for word in sentence] for sentence in sentences]\n",
    "    stop_words = set(stopwords.words(\"polish.txt\"))\n",
    "    stop_words.update(set(stopwords.words('english')))\n",
    "    tokenized_sentences = [[word for word in sentence if word and word not in stop_words] for sentence in\n",
    "                           tokenized_sentences]\n",
    "\n",
    "    new_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_tokenized_sentences.append(sentence)\n",
    "\n",
    "    return new_tokenized_sentences\n",
    "\n",
    "\n",
    "def mapping(sentences):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    word_to_id['null'] = 0  # padding\n",
    "    id_to_word[0] = 'null'  # padding\n",
    "\n",
    "    iterator = 1\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if not token in word_to_id:\n",
    "                word_to_id[token] = iterator\n",
    "                id_to_word[iterator] = token\n",
    "                iterator += 1\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def encode_data(sentences, word_to_idx):\n",
    "    max_len = max(len(lst) for lst in sentences)\n",
    "    encoded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            encoded_sentence.append(word_to_idx[word])\n",
    "        while len(encoded_sentence) < max_len:\n",
    "            encoded_sentence.append(0)\n",
    "\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "    return encoded_sentences\n",
    "\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0] * num_ns, dtype=\"int64\")\n",
    "\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels\n",
    "\n",
    "\n",
    "def cosine_similarity(first_vector, second_vector):\n",
    "    return np.dot(first_vector, second_vector) / (np.linalg.norm(first_vector) * np.linalg.norm(second_vector))\n",
    "\n",
    "def predict_similar_words(model, text, words, ids, n_words_to_predict):\n",
    "    idx_text = words[text]\n",
    "    embeddings = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "    text_vector = embeddings[idx_text]\n",
    "\n",
    "    cosine_sim_dict = {word_embedding_idx: cosine_similarity(text_vector, embedding) for word_embedding_idx, embedding in enumerate(embeddings)}\n",
    "    del cosine_sim_dict[idx_text]\n",
    "\n",
    "    sorted_cosine_sim_list = sorted(cosine_sim_dict.items(), key=lambda x:x[1])[::-1]\n",
    "    words_to_return = [(ids[sorted_cosine_sim_list[word][0]], sorted_cosine_sim_list[word][1]) for word in range(n_words_to_predict)]\n",
    "\n",
    "    return words_to_return\n",
    "\n",
    "class MyWord2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(MyWord2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                                 embedding_dim,\n",
    "                                                 input_length=1,\n",
    "                                                 name=\"w2v_embedding\")\n",
    "\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                                  embedding_dim,\n",
    "                                                  input_length=num_ns + 1)\n",
    "\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        output = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "file_path = './polish_corpus/polish_tokenized_sentences.pkl'\n",
    "polish_tokenized_sentences = load_tokenized_sentences(file_path)\n",
    "\n",
    "preprocessed = preprocess_text_unpacked(polish_tokenized_sentences)\n",
    "words, ids = mapping(preprocessed)\n",
    "\n",
    "encoded_data = encode_data(preprocessed, words)\n",
    "\n",
    "vocab_size = len(words)\n",
    "SEED = 2137\n",
    "num_ns = 10\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "window_size = 3\n",
    "NAME = 'polish_model'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:30:25.842269Z",
     "end_time": "2023-04-05T18:30:29.026180Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=encoded_data,\n",
    "    window_size=window_size,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T13:29:03.608347Z",
     "end_time": "2023-04-03T13:29:03.621347Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# tf.data.Dataset.save(\n",
    "#     dataset, NAME, compression=None, shard_func=None, checkpoint_args=None\n",
    "# )\n",
    "\n",
    "dataset = tf.data.Dataset.load(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:30:32.096429Z",
     "end_time": "2023-04-05T18:30:34.572946Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# embedding_dim = 200\n",
    "# word2vec = MyWord2Vec(vocab_size, embedding_dim)\n",
    "# word2vec.compile(optimizer='adam',\n",
    "#                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "word2vec = tf.keras.models.load_model(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:30:34.569948Z",
     "end_time": "2023-04-05T18:30:40.988458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2330/2330 [==============================] - 990s 425ms/step - loss: 1.5106 - accuracy: 0.4905\n",
      "Epoch 2/10\n",
      "2330/2330 [==============================] - 984s 422ms/step - loss: 1.1358 - accuracy: 0.5784\n",
      "Epoch 3/10\n",
      "2330/2330 [==============================] - 1003s 430ms/step - loss: 0.9426 - accuracy: 0.6678\n",
      "Epoch 4/10\n",
      "2330/2330 [==============================] - 983s 422ms/step - loss: 0.6348 - accuracy: 0.8231\n",
      "Epoch 5/10\n",
      "2330/2330 [==============================] - 975s 419ms/step - loss: 0.3408 - accuracy: 0.9383\n",
      "Epoch 6/10\n",
      "2330/2330 [==============================] - 1001s 429ms/step - loss: 0.1567 - accuracy: 0.9811\n",
      "Epoch 7/10\n",
      "2330/2330 [==============================] - 976s 419ms/step - loss: 0.0689 - accuracy: 0.9935\n",
      "Epoch 8/10\n",
      "2330/2330 [==============================] - 962s 413ms/step - loss: 0.0316 - accuracy: 0.9973\n",
      "Epoch 9/10\n",
      "2330/2330 [==============================] - 961s 412ms/step - loss: 0.0158 - accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "2330/2330 [==============================] - 960s 412ms/step - loss: 0.0088 - accuracy: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_model\\assets\n"
     ]
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=10)\n",
    "word2vec.save(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T19:10:33.053249Z",
     "end_time": "2023-04-04T21:53:56.204360Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vectors = io.open(NAME + '/vectors.tsv', 'w', encoding='utf-8')\n",
    "metadata = io.open(NAME + '/metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    vectors.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    metadata.write(word + \"\\n\")\n",
    "\n",
    "vectors.close()\n",
    "metadata.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:30:40.993460Z",
     "end_time": "2023-04-05T18:31:00.661869Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[('tekille', 0.42743945),\n ('ulotkami', 0.40382966),\n ('trojgiem', 0.3977806),\n ('ucztują', 0.39254016),\n ('parskanie', 0.39170533)]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_similar_words(word2vec, 'miasto', words, ids, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:34:30.514170Z",
     "end_time": "2023-04-05T18:34:32.230715Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('usa', 0.9998107552528381), ('nadal', 0.9997984766960144), ('względu', 0.9997936487197876), ('system', 0.999787449836731), ('wyłącznie', 0.9997779130935669)]\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences=preprocessed, vector_size=100, window=5, min_count=5, workers=4, epochs=5)\n",
    "print(model.wv.most_similar(\"miasto\", topn=5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:34:40.888924Z",
     "end_time": "2023-04-05T18:34:43.804269Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# Access the sentences in the Brown corpus\n",
    "sentences = brown.sents()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:34:54.875863Z",
     "end_time": "2023-04-05T18:34:55.044656Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "preprocessed_brown = preprocess_text_unpacked(sentences)\n",
    "words_brown, ids_brown = mapping(preprocessed_brown)\n",
    "\n",
    "encoded_data_brown = encode_data(preprocessed_brown, words_brown)\n",
    "\n",
    "vocab_size = len(words_brown)\n",
    "SEED = 2137\n",
    "num_ns = 4\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "window_size = 2\n",
    "NAME = 'brown_model'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:34:58.106179Z",
     "end_time": "2023-04-05T18:35:02.896217Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57340/57340 [06:22<00:00, 149.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (1265070,)\n",
      "contexts.shape: (1265070, 5)\n",
      "labels.shape: (1265070, 5)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=encoded_data_brown,\n",
    "    window_size=window_size,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# tf.data.Dataset.save(\n",
    "#     dataset, NAME, compression=None, shard_func=None, checkpoint_args=None\n",
    "# )\n",
    "\n",
    "dataset = tf.data.Dataset.load(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:35:02.915220Z",
     "end_time": "2023-04-05T18:35:02.959226Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# embedding_dim = 200\n",
    "# word2vec = MyWord2Vec(vocab_size, embedding_dim)\n",
    "# word2vec.compile(optimizer='adam',\n",
    "#                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "word2vec = tf.keras.models.load_model(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:35:02.963225Z",
     "end_time": "2023-04-05T18:35:05.682701Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1235/1235 [==============================] - 185s 148ms/step - loss: 1.0194 - accuracy: 0.6211\n",
      "Epoch 2/10\n",
      "1235/1235 [==============================] - 191s 154ms/step - loss: 0.7504 - accuracy: 0.7080\n",
      "Epoch 3/10\n",
      "1235/1235 [==============================] - 188s 152ms/step - loss: 0.6579 - accuracy: 0.7466\n",
      "Epoch 4/10\n",
      "1235/1235 [==============================] - 191s 155ms/step - loss: 0.5465 - accuracy: 0.8027\n",
      "Epoch 5/10\n",
      "1235/1235 [==============================] - 191s 155ms/step - loss: 0.4269 - accuracy: 0.8618\n",
      "Epoch 6/10\n",
      "1235/1235 [==============================] - 199s 161ms/step - loss: 0.3218 - accuracy: 0.9083\n",
      "Epoch 7/10\n",
      "1235/1235 [==============================] - 201s 163ms/step - loss: 0.2415 - accuracy: 0.9379\n",
      "Epoch 8/10\n",
      "1235/1235 [==============================] - 199s 161ms/step - loss: 0.1844 - accuracy: 0.9557\n",
      "Epoch 9/10\n",
      "1235/1235 [==============================] - 199s 161ms/step - loss: 0.1448 - accuracy: 0.9667\n",
      "Epoch 10/10\n",
      "1235/1235 [==============================] - 198s 160ms/step - loss: 0.1172 - accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: brown_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: brown_model\\assets\n"
     ]
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=10)\n",
    "word2vec.save(NAME)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vectors = io.open(NAME + '/vectors.tsv', 'w', encoding='utf-8')\n",
    "metadata = io.open(NAME + '/metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_brown):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    vectors.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    metadata.write(word + \"\\n\")\n",
    "vectors.close()\n",
    "metadata.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[('constancy', 0.5237366),\n ('gawky', 0.522119),\n ('stripped', 0.51077664),\n ('niger', 0.4970776),\n ('rallying', 0.49339056)]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_similar_words(word2vec, 'national', words_brown, ids_brown, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:35:05.685702Z",
     "end_time": "2023-04-05T18:35:06.287613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('program', 0.9960526823997498), ('american', 0.99165940284729), ('local', 0.9915347695350647), ('development', 0.9897612929344177), ('department', 0.9890535473823547)]\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences=preprocessed_brown, vector_size=100, window=5, min_count=5, workers=4, epochs=5)\n",
    "print(model.wv.most_similar(\"national\", topn=5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:35:23.430574Z",
     "end_time": "2023-04-05T18:35:25.739797Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "# import nltk\n",
    "# nltk.download('reuters')\n",
    "sentences = reuters.sents()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:35:35.437586Z",
     "end_time": "2023-04-05T18:35:35.891663Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "preprocessed_reuters = preprocess_text_unpacked(sentences)\n",
    "words_reuters, ids_reuters = mapping(preprocessed_reuters)\n",
    "\n",
    "encoded_data_reuters = encode_data(preprocessed_reuters, words_reuters)\n",
    "\n",
    "vocab_size = len(words_reuters)\n",
    "SEED = 2137\n",
    "num_ns = 4\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "NAME = 'reuters_model'\n",
    "window_size = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:35:44.039265Z",
     "end_time": "2023-04-05T18:35:58.924473Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54716/54716 [07:49<00:00, 116.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (1481282,)\n",
      "contexts.shape: (1481282, 11)\n",
      "labels.shape: (1481282, 11)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=encoded_data_reuters,\n",
    "    window_size=window_size,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "#dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "#tf.data.Dataset.save(\n",
    " #   dataset, NAME, compression=None, shard_func=None, checkpoint_args=None\n",
    "#)\n",
    "\n",
    "dataset = tf.data.Dataset.load(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:35:58.926473Z",
     "end_time": "2023-04-05T18:35:58.971472Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# embedding_dim = 200\n",
    "# word2vec = MyWord2Vec(vocab_size, embedding_dim)\n",
    "# word2vec.compile(optimizer='adam',\n",
    "#                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "word2vec = tf.keras.models.load_model(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:35:58.958474Z",
     "end_time": "2023-04-05T18:36:00.642598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1446/1446 [==============================] - 155s 107ms/step - loss: 0.2031 - accuracy: 0.9320\n",
      "Epoch 2/10\n",
      "1446/1446 [==============================] - 156s 108ms/step - loss: 0.1989 - accuracy: 0.9329\n",
      "Epoch 3/10\n",
      "1446/1446 [==============================] - 155s 107ms/step - loss: 0.1952 - accuracy: 0.9336\n",
      "Epoch 4/10\n",
      "1446/1446 [==============================] - 155s 108ms/step - loss: 0.1920 - accuracy: 0.9343\n",
      "Epoch 5/10\n",
      "1446/1446 [==============================] - 156s 108ms/step - loss: 0.1891 - accuracy: 0.9349\n",
      "Epoch 6/10\n",
      "1446/1446 [==============================] - 157s 109ms/step - loss: 0.1865 - accuracy: 0.9354\n",
      "Epoch 7/10\n",
      "1446/1446 [==============================] - 156s 108ms/step - loss: 0.1843 - accuracy: 0.9358\n",
      "Epoch 8/10\n",
      "1446/1446 [==============================] - 156s 108ms/step - loss: 0.1822 - accuracy: 0.9362\n",
      "Epoch 9/10\n",
      "1446/1446 [==============================] - 160s 111ms/step - loss: 0.1804 - accuracy: 0.9365\n",
      "Epoch 10/10\n",
      "1446/1446 [==============================] - 156s 108ms/step - loss: 0.1787 - accuracy: 0.9368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_1 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as w2v_embedding_layer_call_fn, w2v_embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reuters_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reuters_model\\assets\n"
     ]
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=10)\n",
    "word2vec.save(NAME)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vectors = io.open(NAME + '/vectors.tsv', 'w', encoding='utf-8')\n",
    "metadata = io.open(NAME + '/metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_reuters):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    vectors.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    metadata.write(word + \"\\n\")\n",
    "vectors.close()\n",
    "metadata.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[('elbe', 0.44245476),\n ('rotting', 0.40129274),\n ('sour', 0.3954809),\n ('sfb', 0.39009702),\n ('bms', 0.38344958)]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_similar_words(word2vec, 'major', words_reuters, ids_reuters, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:36:00.659357Z",
     "end_time": "2023-04-05T18:36:00.989901Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('including', 0.9991880059242249), ('companies', 0.9987926483154297), ('division', 0.9986330270767212), ('courses', 0.9984760284423828), ('dallas', 0.9983601570129395)]\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences=preprocessed_brown, vector_size=100, window=5, min_count=5, workers=4, epochs=5)\n",
    "print(model.wv.most_similar(\"major\", topn=5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:36:00.990903Z",
     "end_time": "2023-04-05T18:36:02.797984Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load(\"text8\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:36:11.143038Z",
     "end_time": "2023-04-05T18:36:11.446442Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "preprocessed_text8 = preprocess_text_unpacked(list(dataset)[:100])\n",
    "words_text8, ids_text8 = mapping(preprocessed_text8)\n",
    "\n",
    "encoded_data_text8 = encode_data(preprocessed_text8, words_text8)\n",
    "\n",
    "vocab_size = len(words_text8)\n",
    "SEED = 2137\n",
    "num_ns = 10\n",
    "BATCH_SIZE = 1000\n",
    "BUFFER_SIZE = 10000\n",
    "NAME = 'text8_model'\n",
    "window_size = 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:36:11.693732Z",
     "end_time": "2023-04-05T18:36:15.668166Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:40<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (1305986,)\n",
      "contexts.shape: (1305986, 11)\n",
      "labels.shape: (1305986, 11)\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=encoded_data_text8,\n",
    "    window_size=window_size,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:28:17.194192Z",
     "end_time": "2023-04-05T00:35:12.241734Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# tf.data.Dataset.save(\n",
    "#    dataset, NAME, compression=None, shard_func=None, checkpoint_args=None\n",
    "# )\n",
    "\n",
    "dataset = tf.data.Dataset.load(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:36:18.720261Z",
     "end_time": "2023-04-05T18:36:18.758257Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# embedding_dim = 200\n",
    "# word2vec = MyWord2Vec(vocab_size, embedding_dim)\n",
    "# word2vec.compile(optimizer='adam',\n",
    "#                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "word2vec = tf.keras.models.load_model(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:36:22.422819Z",
     "end_time": "2023-04-05T18:36:24.886708Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1305/1305 [==============================] - 233s 178ms/step - loss: 1.6725 - accuracy: 0.4500\n",
      "Epoch 2/10\n",
      "1305/1305 [==============================] - 231s 177ms/step - loss: 1.3034 - accuracy: 0.5402\n",
      "Epoch 3/10\n",
      "1305/1305 [==============================] - 230s 176ms/step - loss: 1.1391 - accuracy: 0.6030\n",
      "Epoch 4/10\n",
      "1305/1305 [==============================] - 232s 178ms/step - loss: 0.9084 - accuracy: 0.7056\n",
      "Epoch 5/10\n",
      "1305/1305 [==============================] - 232s 178ms/step - loss: 0.6619 - accuracy: 0.8150\n",
      "Epoch 6/10\n",
      "1305/1305 [==============================] - 231s 177ms/step - loss: 0.4595 - accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "1305/1305 [==============================] - 228s 175ms/step - loss: 0.3174 - accuracy: 0.9328\n",
      "Epoch 8/10\n",
      "1305/1305 [==============================] - 227s 174ms/step - loss: 0.2238 - accuracy: 0.9559\n",
      "Epoch 9/10\n",
      "1305/1305 [==============================] - 229s 175ms/step - loss: 0.1624 - accuracy: 0.9697\n",
      "Epoch 10/10\n",
      "1305/1305 [==============================] - 234s 179ms/step - loss: 0.1211 - accuracy: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: text8_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: text8_model\\assets\n"
     ]
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=10)\n",
    "word2vec.save(NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T00:35:18.983461Z",
     "end_time": "2023-04-05T01:13:50.912448Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "[('approaches', 0.40962726),\n ('tailor', 0.4086811),\n ('monarchies', 0.40019533),\n ('kraftwerk', 0.3947389),\n ('complementary', 0.3904963)]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_similar_words(word2vec, 'intervention', words_text8, ids_text8, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:36:29.110531Z",
     "end_time": "2023-04-05T18:36:29.924817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('policies', 0.9976128935813904), ('politicians', 0.9968527555465698), ('somalia', 0.9967787265777588), ('socialist', 0.9964067339897156), ('status', 0.9960634112358093)]\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences=preprocessed_text8, vector_size=100, window=5, min_count=5, workers=4, epochs=5)\n",
    "print(model.wv.most_similar(\"intervention\", topn=5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T18:36:42.470848Z",
     "end_time": "2023-04-05T18:36:44.755883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
