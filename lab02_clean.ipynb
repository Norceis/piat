{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Code created using ChatGPT-4, no promises it works correctly\n",
    "\n",
    "import glob\n",
    "from lxml import etree\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def read_and_tokenize_polish_corpus(corpus_folder):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    # Iterate over all XML files in the corpus folder\n",
    "    for file_path in glob.glob(f\"{corpus_folder}/**/*.xml\", recursive=True):\n",
    "        # Parse the XML file\n",
    "        tree = etree.parse(file_path)\n",
    "\n",
    "        # Retrieve the XML namespaces and register a default namespace\n",
    "        namespaces = tree.getroot().nsmap.copy()\n",
    "        namespaces['default'] = namespaces.pop(None)\n",
    "\n",
    "        # Extract all text content from the <ab> elements\n",
    "        abs = tree.xpath(\"//default:ab\", namespaces=namespaces)\n",
    "        for ab in abs:\n",
    "            text = \" \".join(ab.xpath(\".//text()\"))\n",
    "\n",
    "            # Tokenize sentences and words\n",
    "            if text:\n",
    "                for sentence in sent_tokenize(text, language=\"polish\"):\n",
    "                    tokenized_sentences.append(word_tokenize(sentence, language=\"polish\"))\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "polish_tokenized_sentences = read_and_tokenize_polish_corpus(\"polish_corpus\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_ccl(xml_string):\n",
    "    root = ET.fromstring(xml_string)\n",
    "    sentences = []\n",
    "\n",
    "    for chunk in root.findall('chunk'):\n",
    "        for sentence in chunk.findall('sentence'):\n",
    "            sentence_text = ''\n",
    "\n",
    "            for tok in sentence.findall('tok'):\n",
    "                orth = tok.find('orth').text\n",
    "                sentence_text += orth + ' '\n",
    "\n",
    "            sentences.append(sentence_text.strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def process_directory(path):\n",
    "    sentencesList = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as xml_file:\n",
    "                    xml_string = xml_file.read()\n",
    "                    sentences = parse_ccl(xml_string)\n",
    "                    for sentence in sentences:\n",
    "                        sentencesList.append(sentence)\n",
    "    return sentencesList\n",
    "\n",
    "\n",
    "\n",
    "def read_and_tokenize_polish_corpus_kpwr(corpus_folder):\n",
    "\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    sentences = process_directory(corpus_folder)\n",
    "    for sentence in sentences:\n",
    "      tokenized_sentences.append(word_tokenize(sentence, language=\"polish\"))\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "corpus_folder = './polish_corpus_kpwr'\n",
    "kpwr_tokenized_sentences = read_and_tokenize_polish_corpus_kpwr(corpus_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_tokenized_sentences(tokenized_sentences, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(tokenized_sentences, file)\n",
    "\n",
    "def load_tokenized_sentences(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        tokenized_sentences = pickle.load(file)\n",
    "    return tokenized_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "save_tokenized_sentences(polish_tokenized_sentences, './polish_corpus/polish_tokenized_sentences.pkl')\n",
    "save_tokenized_sentences(kpwr_tokenized_sentences, './polish_corpus/kpwr_tokenized_sentences.pkl')\n",
    "\n",
    "tokenized_sentences = []\n",
    "tokenized_sentences.extend(polish_tokenized_sentences)\n",
    "tokenized_sentences.extend(kpwr_tokenized_sentences)\n",
    "save_tokenized_sentences(tokenized_sentences, './polish_corpus/tokenized_sentences.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pickle\n",
    "tokenized_sentences = load_tokenized_sentences('./polish_corpus/tokenized_sentences.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocess_text(sentences):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation) # removing punctuation - it has to be a conscious decision\n",
    "    tokenized_sentences = [[word.lower().translate(table) for word in sentence] for sentence in sentences]\n",
    "    new_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        new_sentence = []\n",
    "        for x in sentence:\n",
    "            if x != '':\n",
    "                new_sentence.append(x)\n",
    "        new_tokenized_sentences.append(' '.join(new_sentence))\n",
    "\n",
    "    removed_empty_sentences = []\n",
    "    for sentence in new_tokenized_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            removed_empty_sentences.append(sentence)\n",
    "\n",
    "    return removed_empty_sentences\n",
    "\n",
    "preprocessed_tokenized_sentences = preprocess_text(tokenized_sentences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "110438"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_tokenized_sentences)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3107/3107 [==============================] - 948s 304ms/step - loss: 0.1740 - val_loss: 0.1909\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x223a22c1850>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word-level generation and prediction\n",
    "# Toy-problem, not using the previous corpuses\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the Word2Vec model and text corpus data\n",
    "word2vec_model = Word2Vec.load(\"word2vec_large_model.model\")\n",
    "# corpus = ['This is the second sentence to train the model', 'This is the first sentence', 'This is the third sentence just because']\n",
    "corpus = preprocessed_tokenized_sentences\n",
    "embedding_dim = 100\n",
    "\n",
    "\n",
    "def extract_target_words(corpus):\n",
    "    target_words = []\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        target_word = words[-1]  # Get the last word in the sentence\n",
    "        target_words.append(target_word)\n",
    "    return target_words\n",
    "\n",
    "target_words = extract_target_words(corpus)\n",
    "\n",
    "# Create target_vectors\n",
    "target_vectors = np.zeros((len(target_words), embedding_dim))\n",
    "\n",
    "for idx, word in enumerate(target_words):\n",
    "    try:\n",
    "        target_vectors[idx] = word2vec_model.wv[word]\n",
    "    except KeyError:\n",
    "        target_vectors[idx] = np.random.rand(embedding_dim)  # initialize randomly\n",
    "\n",
    "# Transform each word in the corpus into a corresponding vector representation\n",
    "corpus_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_vectors = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            sentence_vectors.append(word2vec_model.wv[word])\n",
    "        except KeyError:\n",
    "            sentence_vectors.append(np.random.rand(100))  # initialize randomly\n",
    "    corpus_vectors.append(sentence_vectors)\n",
    "\n",
    "# Pad each sentence in the corpus to a fixed length, as required by the LSTM model\n",
    "'''\n",
    "Padding is performed by adding filler values (usually zeros) to the sequences until they reach the desired length.\n",
    "Padding is necessary for LSTMs (Long Short-Term Memory) that process input sequences of a fixed length.\n",
    "\n",
    "\n",
    "There are two common padding strategies:\n",
    "Pre-padding: Filler values are added to the beginning of the sequence.\n",
    "Post-padding: Filler values are added to the end of the sequence.\n",
    "\n",
    "The max_len variable is set to the length of the longest sequence in the corpus. B\n",
    "y padding all sequences to this length, we ensure that the input to the LSTM model is\n",
    "standardized and can be processed correctly.\n",
    "\n",
    "It's important to note that padding can affect model performance.\n",
    "Very short sequences may be dominated by the padding values, which can introduce\n",
    "noise and make it harder for the model to learn meaningful patterns.\n",
    "On the other hand, excessively long padding can lead to increased computational\n",
    "requirements and memory consumption.\n",
    "Choosing an appropriate padding length based on the dataset and problem is crucial for achieving good performance.\n",
    "'''\n",
    "\n",
    "max_len = max(len(sentence) for sentence in corpus_vectors)\n",
    "corpus_vectors_padded = np.zeros((len(corpus_vectors), max_len, embedding_dim), dtype='float32')\n",
    "\n",
    "for i, sentence_vectors in enumerate(corpus_vectors):\n",
    "    for j, vector in enumerate(sentence_vectors):\n",
    "        corpus_vectors_padded[i, j, :] = vector\n",
    "\n",
    "# Build an LSTM model in Keras with appropriate input and output layers\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len, embedding_dim)))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(embedding_dim))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the LSTM model on the prepared data\n",
    "model.fit(corpus_vectors_padded, target_vectors, epochs=1, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3107/3107 [==============================] - 922s 297ms/step - loss: 0.1728 - val_loss: 0.1911\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2216aaa3190>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(corpus_vectors_padded, target_vectors, epochs=1, batch_size=32, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'plama piętaka jedna spośród kilku najznakomitszych współczesnych powieści także ze względu na jej zaklasyfikowanie wraz z całą twórczością tego pisarza do nurtu wiejskiego nie ma w odbiorze powszechnym tej rangi jaką rzeczywiście posiada'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[25]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "kilku najznakomitszych kosova kosova kosova kosova\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input_text(text, max_len):\n",
    "    # Transform input text into vector representation\n",
    "    input_vectors = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            input_vectors.append(word2vec_model.wv[word])\n",
    "        except KeyError:\n",
    "            input_vectors.append(np.random.rand(embedding_dim))  # initialize randomly\n",
    "\n",
    "    # Pad the input text to the required length\n",
    "    input_vectors_padded = np.zeros((1, max_len, embedding_dim), dtype='float32')\n",
    "    for j, vector in enumerate(input_vectors):\n",
    "        input_vectors_padded[0, j, :] = vector\n",
    "\n",
    "    return input_vectors_padded\n",
    "\n",
    "def generate_text(model, input_text, num_words_to_generate=10):\n",
    "    generated_text = input_text\n",
    "\n",
    "    for _ in range(num_words_to_generate):\n",
    "        # Preprocess and pad the input text\n",
    "        input_vectors_padded = preprocess_input_text(generated_text, max_len)\n",
    "\n",
    "        # Predict the next word using the LSTM model\n",
    "        prediction = model.predict(input_vectors_padded)\n",
    "        predicted_vector = prediction[0]\n",
    "\n",
    "        # Find the corresponding word for the predicted index\n",
    "        predicted_word = word2vec_model.wv.most_similar(positive=[predicted_vector], topn=1)[0][0]\n",
    "\n",
    "        # Append the predicted word to the input text\n",
    "        generated_text += \" \" + predicted_word\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_text = \"kilku najznakomitszych\"\n",
    "num_words_to_generate = 4\n",
    "generated_text = generate_text(model, input_text, num_words_to_generate)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
