{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import glob\n",
    "from lxml import etree\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pickle\n",
    "import functools\n",
    "import operator\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tqdm import tqdm\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "def preprocess_text(sentences):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation) # removing punctuation - it has to be a conscious decision\n",
    "    tokenized_sentences = [[word.lower().translate(table) for word in sentence] for sentence in sentences]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokenized_sentences = [[word for word in sentence if word and word not in stop_words] for sentence in tokenized_sentences]\n",
    "    return tokenized_sentences\n",
    "\n",
    "def save_tokenized_sentences(tokenized_sentences, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(tokenized_sentences, file)\n",
    "\n",
    "def load_tokenized_sentences(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        tokenized_sentences = pickle.load(file)\n",
    "    return tokenized_sentences\n",
    "\n",
    "def preprocess_text_unpacked(sentences):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation) # removing punctuation - it has to be a conscious decision\n",
    "    tokenized_sentences = [[word.lower().translate(table) for word in sentence] for sentence in sentences]\n",
    "    stop_words = set(stopwords.words(\"polish.txt\"))\n",
    "    tokenized_sentences = [[word for word in sentence if word and word not in stop_words] for sentence in tokenized_sentences]\n",
    "    unpacked = functools.reduce(operator.iconcat, tokenized_sentences, [])\n",
    "    return unpacked\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "\n",
    "    X = np.array(X)\n",
    "    # X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    # Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def batcherize(x_train, y_train, n_batches, n_batch):\n",
    "\n",
    "    vocab_size = np.max(x_train) + 1\n",
    "    dataset_length = x_train.shape[0]\n",
    "    batch_size = int(np.floor(dataset_length / n_batches))\n",
    "\n",
    "    y_train_batch = np.zeros((batch_size, vocab_size))\n",
    "    y_train_batch[np.arange(batch_size), y_train[batch_size * n_batch : (n_batch + 1) * batch_size]] = 1\n",
    "\n",
    "    x_train_batch = np.zeros((batch_size, vocab_size))\n",
    "    x_train_batch[np.arange(batch_size), x_train[batch_size * n_batch : (n_batch + 1) * batch_size]] = 1\n",
    "\n",
    "    return x_train_batch, y_train_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2\n",
    "\n",
    "Implement the Word2Vec algorithm, using information provided during lecture. Test it on the brown and text8 corpora (corpuses?)\n",
    "\n",
    "Next test the implementation on the Polish language corpus, check the correctness by checking token similiarity and vector arithmetics\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "POLISH DATASET -----------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "file_path = './polish_corpus/polish_tokenized_sentences.pkl'\n",
    "polish_tokenized_sentences = load_tokenized_sentences(file_path)\n",
    "\n",
    "preprocessed = preprocess_text_unpacked(polish_tokenized_sentences)\n",
    "words, ids = mapping(preprocessed)\n",
    "\n",
    "x_train, y_train = generate_training_data(preprocessed, words, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 50)                6494100   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 129881)            6623931   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,118,031\n",
      "Trainable params: 13,118,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(2137)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Input(shape=(np.max(x_train) + 1, )))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(np.max(x_train) + 1, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Latest accuracy: 0.003937007859349251:  12%|█▏        | 7/57 [00:20<02:05,  2.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n",
      "Latest accuracy: 0.0019685039296746254:  30%|██▉       | 17/57 [00:47<01:43,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n",
      "Latest accuracy: 0.0019685039296746254:  47%|████▋     | 27/57 [01:14<01:14,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n",
      "Latest accuracy: 0.0019685039296746254:  65%|██████▍   | 37/57 [01:40<00:49,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n",
      "Latest accuracy: 0.011811023578047752:  82%|████████▏ | 47/57 [02:08<00:26,  2.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: polish_keras_model\\assets\n",
      "Latest accuracy: 0.011811023578047752:  88%|████████▊ | 50/57 [02:15<00:18,  2.70s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (508,) (138,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m pbar \u001B[38;5;241m=\u001B[39m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m4943\u001B[39m, n_batches))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[1;32m----> 6\u001B[0m     x_train_batch, y_train_batch \u001B[38;5;241m=\u001B[39m \u001B[43mbatcherize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     hist \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(x_train_batch, y_train_batch, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      8\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLatest accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhist\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 92\u001B[0m, in \u001B[0;36mbatcherize\u001B[1;34m(x_train, y_train, n_batches, n_batch)\u001B[0m\n\u001B[0;32m     89\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(np\u001B[38;5;241m.\u001B[39mceil(dataset_length \u001B[38;5;241m/\u001B[39m n_batches))\n\u001B[0;32m     91\u001B[0m y_train_batch \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((batch_size, train_size))\n\u001B[1;32m---> 92\u001B[0m \u001B[43my_train_batch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mn_batch\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_batch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     94\u001B[0m x_train_batch \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((batch_size, train_size))\n\u001B[0;32m     95\u001B[0m x_train_batch[np\u001B[38;5;241m.\u001B[39marange(batch_size), x_train[batch_size \u001B[38;5;241m*\u001B[39m n_batch : (n_batch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m batch_size]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mIndexError\u001B[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (508,) (138,) "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('polish_keras_model')\n",
    "\n",
    "n_batches = 5000\n",
    "pbar = tqdm(range(0, n_batches))\n",
    "for batch_idx in pbar:\n",
    "    x_train_batch, y_train_batch = batcherize(x_train, y_train, n_batches, batch_idx)\n",
    "    hist = model.fit(x_train_batch, y_train_batch, verbose=0, epochs=1)\n",
    "    pbar.set_description(f\"Latest accuracy: {hist.history['accuracy'][-1]}\")\n",
    "\n",
    "    if not batch_idx % 10:\n",
    "        model.save('polish_keras_model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "['zaakompaniować',\n 'jedynej',\n 'uczennicy',\n 'papy',\n 'moment',\n 'musical',\n 'szuberta',\n 'jedynej',\n 'uczennicy',\n 'pierwszej']"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[280:290]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('polish_keras_model')\n",
    "predict_id = words['jedynej']\n",
    "test_sample = np.zeros((1, np.max(x_train)))\n",
    "test_sample[0, predict_id] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    }
   ],
   "source": [
    "model_prediction = model.predict(test_sample)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "prediction = np.argsort(model_prediction[0])[-5:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "sorted_descending_prediction = prediction[::-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([40456, 17036, 77998,  8574, 80629], dtype=int64)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_descending_prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opodal\n",
      "serbię\n",
      "kapłanów\n",
      "uniwerku\n",
      "poważniejszy\n"
     ]
    }
   ],
   "source": [
    "for idx in sorted_descending_prediction:\n",
    "    print(ids[idx])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
