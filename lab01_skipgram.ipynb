{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import glob\n",
    "from lxml import etree\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pickle\n",
    "import functools\n",
    "import operator\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tqdm import tqdm\n",
    "import absl.logging\n",
    "\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def save_tokenized_sentences(tokenized_sentences, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(tokenized_sentences, file)\n",
    "\n",
    "def load_tokenized_sentences(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        tokenized_sentences = pickle.load(file)\n",
    "    return tokenized_sentences\n",
    "\n",
    "def preprocess_text_unpacked(sentences):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation) # removing punctuation - it has to be a conscious decision\n",
    "    tokenized_sentences = [[word.lower().translate(table) for word in sentence] for sentence in sentences]\n",
    "    stop_words = set(stopwords.words(\"polish.txt\"))\n",
    "    tokenized_sentences = [[word for word in sentence if word and word not in stop_words] for sentence in tokenized_sentences]\n",
    "    # tokenized_sentences = functools.reduce(operator.iconcat, tokenized_sentences, [])\n",
    "    return tokenized_sentences\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "\n",
    "    X = np.array(X)\n",
    "    # X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    # Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def batcherize(x_train, y_train, n_batches, n_batch):\n",
    "\n",
    "    vocab_size = np.max(x_train) + 1\n",
    "    dataset_length = x_train.shape[0]\n",
    "    batch_size = int(np.floor(dataset_length / n_batches))\n",
    "\n",
    "    y_train_batch = np.zeros((batch_size, vocab_size))\n",
    "    y_train_batch[np.arange(batch_size), y_train[batch_size * n_batch : (n_batch + 1) * batch_size]] = 1\n",
    "\n",
    "    x_train_batch = np.zeros((batch_size, vocab_size))\n",
    "    x_train_batch[np.arange(batch_size), x_train[batch_size * n_batch : (n_batch + 1) * batch_size]] = 1\n",
    "\n",
    "    return x_train_batch, y_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[['zatrzasnął',\n  'drzwi',\n  'mieszkania',\n  'razy',\n  'przekręcił',\n  'klucz',\n  'nacisnął',\n  'klamkę',\n  'sprawdzić',\n  'zamknięte',\n  'zbiegł',\n  'schodach',\n  'minął',\n  'furtkę',\n  'zamknął',\n  'znalazł',\n  'wąskiej',\n  'uliczce',\n  'ogródkami',\n  'drzemały',\n  'majowym',\n  'słońcu',\n  'trójkątne',\n  'ciemnozielone',\n  'świerki',\n  'jakich',\n  'pobliżu',\n  'domu'],\n ['bohaterem', 'powieści', 'paźniewskiego', 'miasto', 'krzemieniec'],\n ['czasów', 'słowackiego', 'funkcjonuje', 'liceum', 'płynie', 'ikwa'],\n ['krzemieniec',\n  'powieściowy',\n  'tamtym',\n  'krzemieńcem',\n  'miastem',\n  'wywołanym',\n  'osobistej',\n  'pamięci',\n  'paźniewskiego'],\n ['swoją', 'drogę', 'miasta', 'autor', 'krótkich', 'dni', 'zaczął', 'daleka']]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './polish_corpus/polish_tokenized_sentences.pkl'\n",
    "polish_tokenized_sentences = load_tokenized_sentences(file_path)\n",
    "\n",
    "preprocessed = preprocess_text_unpacked(polish_tokenized_sentences)\n",
    "# words, ids = mapping(preprocessed)\n",
    "preprocessed[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88760/88760 [00:35<00:00, 2482.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "101/101 [==============================] - 69s 659ms/step - loss: 0.5253 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "101/101 [==============================] - 67s 667ms/step - loss: 0.4550 - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "101/101 [==============================] - 66s 652ms/step - loss: 0.4481 - accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "101/101 [==============================] - 67s 662ms/step - loss: 0.4433 - accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "101/101 [==============================] - 66s 657ms/step - loss: 0.4402 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class myWord2Vec:\n",
    "    def __init__(self, sentences, window, epochs, embedding_dim = 5, negative_samples = 0, unique = False) -> None:\n",
    "        self.sentences = sentences\n",
    "        self.window = window\n",
    "        self.epochs = epochs\n",
    "        self.negative_samples = negative_samples\n",
    "        self._create_data()\n",
    "        self.model = self._create_model(embedding_dim)\n",
    "        self.unique = unique\n",
    "\n",
    "\n",
    "    def _create_data(self):\n",
    "        lst = []\n",
    "        for x in self.sentences:\n",
    "            lst += x\n",
    "\n",
    "        unique_words = list(set(lst))\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word  = {}\n",
    "        for i, word in enumerate(unique_words):\n",
    "            self.word_to_id[word] = i\n",
    "            self.id_to_word[i] = word\n",
    "\n",
    "        self.vocab_size = len(self.word_to_id)\n",
    "\n",
    "\n",
    "    def _create_model(self, embedding_dim):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(self.vocab_size, embedding_dim, embeddings_initializer=\"RandomNormal\", input_shape=(2,)),\n",
    "            tf.keras.layers.GlobalAveragePooling1D(),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Accuracy()])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def _my_skipgrams(self, sequence, vocabulary_size):\n",
    "        couples = []\n",
    "        labels = []\n",
    "        for i in range(len(sequence)):\n",
    "            wi = sequence[i]\n",
    "            window_start = max(0, i - self.window)\n",
    "            window_end = min(len(sequence), i + self.window + 1)\n",
    "            for j in range(window_start, window_end):\n",
    "                if j != i:\n",
    "                    wj = sequence[j]\n",
    "                    couples.append([wi, wj])\n",
    "                    labels.append(1)\n",
    "\n",
    "        if self.negative_samples > 0:\n",
    "            num_negative_samples = len(labels) * self.negative_samples\n",
    "            words = [couple[0] for couple in couples]\n",
    "            random.shuffle(words)\n",
    "            for i in range(num_negative_samples):\n",
    "                word = words[i % len(words)]\n",
    "                couples.append([word, self.id_to_word[random.randint(1, vocabulary_size - 1)]])\n",
    "                labels.append(0)\n",
    "\n",
    "        return  couples, labels\n",
    "\n",
    "\n",
    "    def _to_one_hot(self, word_index, vocab_size):\n",
    "        one_hot = np.zeros(vocab_size)\n",
    "        one_hot[word_index] = 1\n",
    "        return one_hot\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        y = []\n",
    "        posTrainSet  = []\n",
    "        negTrainSet  = []\n",
    "        for sentence in tqdm(self.sentences):\n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "\n",
    "            couples, labels = self._my_skipgrams(sentence, len(sentence))\n",
    "            for j in range(len(couples)):\n",
    "                couple = couples[j]\n",
    "                if labels[j]:\n",
    "                    posTrainSet.append([self.word_to_id[couple[0]], self.word_to_id[couple[1]]])\n",
    "                    y.append(labels[j])\n",
    "\n",
    "                else:\n",
    "                    negTrainSet.append([self.word_to_id[couple[0]], self.word_to_id[couple[1]]])\n",
    "                    y.append(labels[j])\n",
    "\n",
    "        X = np.concatenate([np.array(posTrainSet), np.array(negTrainSet)], axis=0)\n",
    "        y = np.array(y)\n",
    "        if self.unique:\n",
    "            X = np.unique(X, axis = 0)\n",
    "\n",
    "        self.model.fit(X, y, batch_size=int(X.shape[0]*0.01), epochs=self.epochs)\n",
    "\n",
    "\n",
    "    def find_similar_words(self, input_word, n):\n",
    "        embedding_layer = self.model.layers[0]\n",
    "        word_embeddings = embedding_layer.get_weights()[0]\n",
    "        input_index = self.word_to_id[input_word]\n",
    "        input_embedding = word_embeddings[input_index]\n",
    "        similarities = np.dot(word_embeddings, input_embedding)\n",
    "        most_similar_indices = similarities.argsort()[::-1][1:n+1]\n",
    "        most_similar_words = [list(self.word_to_id.keys())[i] for i in most_similar_indices]\n",
    "        return most_similar_words\n",
    "\n",
    "\n",
    "model = myWord2Vec(sentences=preprocessed, window=3, epochs=5, embedding_dim=250, negative_samples=5)\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['puścili',\n 'dwóch',\n 'rogatki',\n 'trafili',\n 'zagajnik',\n 'śniegiem',\n 'zawalony',\n 'jęli',\n 'otrząsać',\n 'biały',\n 'całun']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[150]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['przemycany', 'chaszczach', 'zagoszczą', '394', 'krzywoprzysięstwo']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.find_similar_words('całun', 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
